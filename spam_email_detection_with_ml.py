# -*- coding: utf-8 -*-
"""Spam email detection with ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14QS4SDgoHA1wi-zHVJ-NdddpIrt5RwXC

Import Libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""Dataset Loading"""

df = pd.read_csv('/content/drive/MyDrive/Spam Email Detection - spam.csv')

from google.colab import drive
drive.mount('/content/drive')

"""Dataset view"""

df.head(10)

"""Dataset rows and column count"""

print('Number of rows:', df.shape[0])
print('Number of columns:', df.shape[1])

"""Dataset Information"""

df.info()

"""Duplicate values"""

dup=df.duplicated().sum()
print('Duplicate values:',dup)

"""Missing or null values"""

df.isnull().sum()

"""Understanding the variable"""

df.describe()

df.columns

"""Check unique values for variables"""

for i in df.columns.to_list():
  print("Number od unique values in ", i, "is", df[i].nunique())

"""Data wrangling"""

df.rename(columns={'v1':'category','v2':'message'},inplace=True)

df.drop(columns={'Unnamed: 2','Unnamed: 3','Unnamed: 4'},inplace=True)

df['spam']=df['category'].apply(lambda x:1 if x=='spam' else 0)

df.head()

"""Chart 1 : Distrubution of ham v/s spam
Data visualisation and charts
"""

spread=df['category'].value_counts()
plt.rcParams['figure.figsize']=(8,5)
spread.plot(kind='pie', autopct='%1.2f%%', cmap='Set1')
plt.title('Distribution of Ham v/s Spam')
plt.show()

"""Chart 2 : Most used words in spam messages."""

df_spam=df[df['category']=='spam'].copy()

from wordcloud import WordCloud, STOPWORDS
comment_words = ''
stopwords = set(STOPWORDS)
for val in df_spam.message:
    val = str(val)
    tokens = val.split()
    for i in range(len(tokens)):
        tokens[i] = tokens[i].lower()
    comment_words += " ".join(tokens)+" "
wordcloud = WordCloud(width = 1000, height = 500,
                background_color ='white',
                stopwords = stopwords,
                min_font_size = 10,
                max_words = 1000,
                colormap = 'gist_heat_r').generate(comment_words)

plt.figure(figsize = (6,6), facecolor = None)
plt.title('Most Used Words In Spam Messages', fontsize = 15, pad=20)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()

"""Data Splitting"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(df.message, df.category, test_size=0.25)

X_train,X_test,y_train,y_test=train_test_split(df.message,df.category,test_size=0.25)

"""ML Model Implementation"""

def evaluate_model(model, X_train, X_test, y_train, y_test):
   model.fit(X_train, y_train)
   y_pred_train = model.predict(X_train)
   y_pred_test = model.predict(X_test)
   pred_prob_train = model.predict_proba(X_train)[:,1]
   pred_prob_test = model.predict_proba(X_test)[:,1]
   roc_auc_train = roc_auc_score(y_train, y_pred_train)
   roc_auc_test = roc_auc_score(y_test, y_pred_test)
   print("\nTrain ROC AUC:", roc_auc_train)
   print("Test ROC AUC:", roc_auc_test)

   fpr_train, tpr_train, thresholds_train = roc_curve(y_train, pred_prob_train)
   fpr_test, tpr_test, thresholds_test = roc_curve(y_test, pred_prob_test)
   plt.plot([0,1],[0,1],'k--')
   plt.plot(fpr_train, tpr_train, label="Train ROC AUC: {:.2f}".format(roc_auc_train))
   plt.plot(fpr_test, tpr_test, label="Test ROC AUC: {:.2f}".format(roc_auc_test))
   plt.legend()
   plt.title("ROC Curve")
   plt.xlabel("False Positive Rate")
   plt.ylabel("True Positive Rate")
   plt.show()
   cm_train = confusion_matrix(y_train, y_pred_train)
   cm_test = confusion_matrix(y_test, y_pred_test)

   fig, ax = plt.subplots(1, 2, figsize=(11,4))

   print("\nConfusion Matrix:")
   sns.heatmap(cm_train, annot=True, xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'], cmap="Oranges", fmt='.4g', ax=ax[0])
   ax[0].set_xlabel("Predicted Label")
   ax[0].set_ylabel("True Label")
   ax[0].set_title("Train Confusion Matrix")

   sns.heatmap(cm_test, annot=True, xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'], cmap="Oranges", fmt='.4g', ax=ax[1])
   ax[1].set_xlabel("Predicted Label")
   ax[1].set_ylabel("True Label")
   ax[1].set_title("Test Confusion Matrix")

   plt.tight_layout()
   plt.show()

   cr_train = classification_report(y_train, y_pred_train, output_dict=True)
   cr_test = classification_report(y_test, y_pred_test, output_dict=True)
   print("\nTrain Classification Report:")
   crt = pd.DataFrame(cr_train).T
   print(crt.to_markdown())
   print("\nTest Classification Report:")
   crt2 = pd.DataFrame(cr_test).T
   print(crt2.to_markdown())
   precision_train = cr_train['weighted avg']['precision']
   precision_test = cr_test['weighted avg']['precision']

   recall_train = cr_train['weighted avg']['recall']
   recall_test = cr_test['weighted avg']['recall']

   acc_train = accuracy_score(y_true = y_train, y_pred = y_pred_train)
   acc_test = accuracy_score(y_true = y_test, y_pred = y_pred_test)

   F1_train = cr_train['weighted avg']['f1-score']
   F1_test = cr_test['weighted avg']['f1-score']

   model_score = [precision_train, precision_test, recall_train, recall_test, acc_train, acc_test, roc_auc_train, roc_auc_test, F1_train, F1_test ]
   return model_score

"""ML Model : Multimonial Naive Bayers"""

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

clf = Pipeline([
    ('vectorizer', CountVectorizer()),
    ('nb', MultinomialNB())
])

clf = Pipeline([
    ('vectorizer', CountVectorizer()),
    ('nb', MultinomialNB())
])

"""Explain ML Model used to evaluating matric score"""

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import roc_auc_score

def evaluate_model(model, X_train, X_test, y_train, y_test):
    from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score

    # Make predictions
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)

    # Calculate metrics
    # Specify pos_label='spam' to indicate the positive class
    precision_train = precision_score(y_train, y_pred_train, pos_label='spam')
    precision_test = precision_score(y_test, y_pred_test, pos_label='spam')
    recall_train = recall_score(y_train, y_pred_train, pos_label='spam')
    recall_test = recall_score(y_test, y_pred_test, pos_label='spam')
    accuracy_train = accuracy_score(y_train, y_pred_train)
    accuracy_test = accuracy_score(y_test, y_pred_test)

    # Get predicted probabilities for ROC AUC calculation
    pred_prob_train = model.predict_proba(X_train)[:,1]
    pred_prob_test = model.predict_proba(X_test)[:,1]

    # Calculate ROC AUC scores
    roc_auc_train = roc_auc_score(y_train, pred_prob_train) # Use pred_prob_train here
    roc_auc_test = roc_auc_score(y_test, pred_prob_test) # Use pred_prob_test here

    #print("\nTrain ROC AUC:", roc_auc_train)
    #print("Test ROC AUC:", roc_auc_test)

    # Specify pos_label='spam' for F1 score as well
    f1_train = f1_score(y_train, y_pred_train, pos_label='spam')
    f1_test = f1_score(y_test, y_pred_test, pos_label='spam')


    return [precision_train, precision_test, recall_train, recall_test, accuracy_train, accuracy_test, roc_auc_train, roc_auc_test, f1_train, f1_test]

# Fit the pipeline to the training data
clf.fit(X_train, y_train)

# evaluate the model
model_score = evaluate_model(clf, X_train, X_test, y_train, y_test)

print("\nModel Score:")
for i, metric in enumerate(['Precision Train', 'Precision Test', 'Recall Train', 'Recall Test', 'Accuracy Train', 'Accuracy Test', 'ROC AUC Train', 'ROC AUC Test', 'F1 Train', 'F1 Test']):
  print(f"{metric}: {model_score[i]:.4f}")

"""Email Spam Detection System"""

def detect_spam(email_text):

    prediction = clf.predict([email_text])

    if prediction == 0:
        return "This is a Ham Email!"
    else:
        return "This is a Spam Email!"

sample_email = 'congratulations you have been'
result = detect_spam(sample_email)
print(result)